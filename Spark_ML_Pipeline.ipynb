{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrtgAv6VZfQo1V0nTHjMMx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dansarmiento/python_analytics_solutions/blob/main/Spark_ML_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create a machine learning pipeline.\n",
        "- Add stages to the pipeline.\n",
        "- Run the pipeline.\n",
        "- Create a machine learning pipeline for regression.\n",
        "- Create a machine learning pipeline for classification."
      ],
      "metadata": {
        "id": "LON9nCf1GWnj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AMn0q4DGOyF",
        "outputId": "a8e5af81-bba5-413d-823d-9de3ab138b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.5 -q\n",
        "!pip install findspark -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# FindSpark simplifies the process of using Apache Spark with Python\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#import functions/Classes for sparkml\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "# import functions/Classes for pipeline creation\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# import functions/Classes for metrics\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
      ],
      "metadata": {
        "id": "x29gP_91GhSK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the session\n",
        "\n",
        "spark = SparkSession.builder.appName(\"ML Pipeline Example\").getOrCreate()"
      ],
      "metadata": {
        "id": "M2OHHjXlGhU2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the data\n",
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acoaj8DkGhXb",
        "outputId": "2364baeb-e2e2-4820-882d-b4ea0d80f4fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 01:47:31--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/mpg.csv\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13891 (14K) [text/csv]\n",
            "Saving to: ‘mpg.csv’\n",
            "\n",
            "mpg.csv             100%[===================>]  13.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-25 01:47:32 (332 MB/s) - ‘mpg.csv’ saved [13891/13891]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the spark.read.csv function we load the data into a dataframe.\n",
        "# the header = True mentions that there is a header row in out csv file\n",
        "# the inferSchema = True, tells spark to automatically find out the data types of the columns.\n",
        "\n",
        "# Load mpg dataset\n",
        "mpg_data = spark.read.csv(\"mpg.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "FspDR7GCGhaI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mpg_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZiHLeMgGhct",
        "outputId": "786739aa-1076-47bb-d759-1b342eae8dd5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- MPG: double (nullable = true)\n",
            " |-- Cylinders: integer (nullable = true)\n",
            " |-- Engine Disp: double (nullable = true)\n",
            " |-- Horsepower: integer (nullable = true)\n",
            " |-- Weight: integer (nullable = true)\n",
            " |-- Accelerate: double (nullable = true)\n",
            " |-- Year: integer (nullable = true)\n",
            " |-- Origin: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define pipeline stages"
      ],
      "metadata": {
        "id": "fqdRwH-3Gt2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stage 1 - assemble the input columns into a single vector\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"Weight\", \"Horsepower\", \"Engine Disp\"], outputCol=\"features\")\n",
        "# Stage 2 - scale the features using standard scaler\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "# Stage 3 - create a linear regression instance\n",
        "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"MPG\")"
      ],
      "metadata": {
        "id": "1tki6yHXGhel"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the pipeline\n",
        "# All the stages of the pipeline are mentioned in the order of execution.\n",
        "pipeline = Pipeline(stages=[vectorAssembler, scaler, lr])"
      ],
      "metadata": {
        "id": "LGIeDavpGhhh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "(trainingData, testData) = mpg_data.randomSplit([0.7, 0.3], seed=42)"
      ],
      "metadata": {
        "id": "cpkxnmuXGhj4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline to the training data\n",
        "# ignore any warnings. The warnings are due to the simplified settings and the security settings of the lab\n",
        "\n",
        "model = pipeline.fit(trainingData)"
      ],
      "metadata": {
        "id": "Z2tce-rWGhmc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "vJPa0WovG8Bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "predictions = model.transform(testData)\n",
        "evaluator = RegressionEvaluator(labelCol=\"MPG\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) =\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1yUPmDjGhrX",
        "outputId": "fa1dce7a-38f6-4b39-da6f-b1e02a1ddf94"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE) = 3.8756646183839187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "2mEl1qt6G_6-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N2BbuN58HCce"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New session for Iris dataset\n",
        "spark = SparkSession.builder.appName(\"ML Pipeline Exercise\").getOrCreate()"
      ],
      "metadata": {
        "id": "wDQz-9kJHCfK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/iris.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U67HGl9zHChh",
        "outputId": "e66c2009-b2df-4b8e-e3c9-f117834ac0fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-25 01:47:51--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0231EN-SkillsNetwork/datasets/iris.csv\n",
            "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.45.118.108\n",
            "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.45.118.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4612 (4.5K) [text/csv]\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "iris.csv            100%[===================>]   4.50K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-25 01:47:51 (1.41 GB/s) - ‘iris.csv’ saved [4612/4612]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris_data = spark.read.csv(\"iris.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "fQuAmtofHCj3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iris_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh59jBrvHCmT",
        "outputId": "d3899e0d-dc23-4894-8d9e-99707e1f7500"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- SepalLengthCm: double (nullable = true)\n",
            " |-- SepalWidthCm: double (nullable = true)\n",
            " |-- PetalLengthCm: double (nullable = true)\n",
            " |-- PetalWidthCm: double (nullable = true)\n",
            " |-- Species: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an indexer stage using StringIndexer that will convert the Species column into a numeric column named \"label\"\n",
        "indexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")"
      ],
      "metadata": {
        "id": "ieEt87AcHCoi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorAssembler stage that creates a feature vector named features using \"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\",\"PetalWidthCm\"\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\",\"PetalWidthCm\"], outputCol=\"features\")"
      ],
      "metadata": {
        "id": "pltzdw7CHCrJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scaler stage that scales the features using standard scaler, name the output columns as scaledFeatures\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")"
      ],
      "metadata": {
        "id": "fOnGxiiTHVjO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a logistic regression stage using featuresCol=\"scaledFeatures\", labelCol=\"label\"\n",
        "classifier = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"label\")"
      ],
      "metadata": {
        "id": "S8KzCn-sHVlc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[indexer,vectorAssembler, scaler, classifier])\n",
        "(trainingData, testData) = iris_data.randomSplit([0.7, 0.3], seed=42)"
      ],
      "metadata": {
        "id": "BaZVN3-CHVoK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(trainingData)"
      ],
      "metadata": {
        "id": "AqaB61JnHVqB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(testData)"
      ],
      "metadata": {
        "id": "PiAjRvN-HVs1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy =\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XHUqohgJHCtl",
        "outputId": "afd3f23f-3ad5-4594-8585-57765ef76081"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9782608695652174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "QoQ0rxKjHmBc"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}