{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX151wfKo2YVE2u6kaqL++",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dansarmiento/analytics_portfolio/blob/main/Apache_Airflow_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install and Configure Airflow\n",
        "\n",
        "This initial step prepares your Google Colab environment to run an Airflow DAG. It begins by installing the apache-airflow package and its necessary dependencies. Once installed, it sets up a dedicated AIRFLOW_HOME directory. This is crucial as it's where Airflow stores its configuration files and the SQLite database that it uses to track DAG runs and task statuses. Finally, the airflow db init command initializes this database, making the environment ready to execute a DAG."
      ],
      "metadata": {
        "id": "LeuSIAZIEr9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START: Updated Installation Cell ---\n",
        "\n",
        "# 1. Uninstall existing versions of pytest and pluggy to ensure a clean state\n",
        "print(\"Uninstalling potentially conflicting packages...\")\n",
        "!pip uninstall -y pytest pluggy\n",
        "\n",
        "# 2. Install a known compatible version of pytest\n",
        "print(\"\\nInstalling a compatible version of pytest...\")\n",
        "!pip install pytest==7.4.4\n",
        "\n",
        "# 3. Now, install Airflow quietly using the constraints file.\n",
        "# This will pull in all other dependencies correctly.\n",
        "print(\"\\nInstalling Apache Airflow...\")\n",
        "!pip install -q apache-airflow==2.8.1 --constraint \"https://raw.githubusercontent.com/apache/airflow/constraints-2.8.1/constraints-3.8.txt\"\n",
        "\n",
        "# 4. Set the AIRFLOW_HOME directory\n",
        "import os\n",
        "os.makedirs('/content/airflow/dags', exist_ok=True)\n",
        "os.environ['AIRFLOW_HOME'] = '/content/airflow'\n",
        "\n",
        "# 5. Initialize the Airflow database\n",
        "print(\"\\nInitializing the Airflow database...\")\n",
        "!airflow db init\n",
        "\n",
        "print(\"\\n--- Airflow installation and initialization complete! ---\")\n",
        "\n",
        "# --- END: Updated Installation Cell ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVcPnY3PDuqd",
        "outputId": "4a04a908-f549-4a7d-e8db-8ead251fea31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uninstalling potentially conflicting packages...\n",
            "Found existing installation: pytest 8.3.5\n",
            "Uninstalling pytest-8.3.5:\n",
            "  Successfully uninstalled pytest-8.3.5\n",
            "Found existing installation: pluggy 1.3.0\n",
            "Uninstalling pluggy-1.3.0:\n",
            "  Successfully uninstalled pluggy-1.3.0\n",
            "\n",
            "Installing a compatible version of pytest...\n",
            "Collecting pytest==7.4.4\n",
            "  Downloading pytest-7.4.4-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.4) (2.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pytest==7.4.4) (23.2)\n",
            "Collecting pluggy<2.0,>=0.12 (from pytest==7.4.4)\n",
            "  Downloading pluggy-1.6.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Downloading pytest-7.4.4-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.3/325.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pluggy-1.6.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pluggy, pytest\n",
            "Successfully installed pluggy-1.6.0 pytest-7.4.4\n",
            "\n",
            "Installing Apache Airflow...\n",
            "\n",
            "Initializing the Airflow database...\n",
            "DB: sqlite:////content/airflow/airflow.db\n",
            "[\u001b[34m2025-05-23T21:46:52.939+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m216} INFO\u001b[0m - Context impl \u001b[01mSQLiteImpl\u001b[22m.\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:46:52.942+0000\u001b[0m] {\u001b[34mmigration.py:\u001b[0m219} INFO\u001b[0m - Will assume \u001b[01mnon-transactional\u001b[22m DDL.\u001b[0m\n",
            "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
            "INFO  [alembic.runtime.migration] Running stamp_revision  -> 88344c1d9134\n",
            "WARNI [airflow.models.crypto] empty cryptography key - values will not be stored encrypted.\n",
            "Initialization done\n",
            "\n",
            "--- Airflow installation and initialization complete! ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create the DAG File\n",
        "\n",
        "Here, we define the entire data engineering pipeline as an Airflow DAG. Using a Colab \"magic command\" (%%writefile), we write the Python code directly into a dag.py file inside the dags folder. This file contains:\n",
        "\n",
        "A DAG definition that sets the schedule and other metadata.\n",
        "BashOperators to perform command-line tasks like creating directories (mkdir), downloading the dataset (wget), and unpacking the tarball (tar).\n",
        "Python Tasks written using the modern @task decorator (TaskFlow API) to handle the data manipulation with the pandas library. These tasks are responsible for extracting data from CSV, TSV, and fixed-width files, consolidating them, and finally transforming the data.\n",
        "\n",
        "Task dependencies (>>) at the end of the file, which explicitly define the order of execution, ensuring the pipeline runs in the correct sequence."
      ],
      "metadata": {
        "id": "Qts6NVO0Dy3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/airflow/dags/etl_pipeline_dag.py\n",
        "import os\n",
        "import pandas as pd\n",
        "import tarfile\n",
        "from datetime import datetime\n",
        "\n",
        "from airflow.decorators import dag, task\n",
        "from airflow.operators.bash import BashOperator\n",
        "\n",
        "# Define file paths for clarity\n",
        "STAGING_DIR = \"/content/airflow/dags/staging\"\n",
        "DOWNLOAD_URL = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/tolldata.tgz\"\n",
        "DOWNLOADED_FILE = os.path.join(STAGING_DIR, \"tolldata.tgz\")\n",
        "EXTRACTED_DIR = STAGING_DIR\n",
        "\n",
        "# Define paths for intermediate and final files\n",
        "CSV_DATA_PATH = os.path.join(STAGING_DIR, 'csv_data.csv')\n",
        "TSV_DATA_PATH = os.path.join(STAGING_DIR, 'tsv_data.csv')\n",
        "FIXED_WIDTH_DATA_PATH = os.path.join(STAGING_DIR, 'fixed_width_data.csv')\n",
        "CONSOLIDATED_DATA_PATH = os.path.join(STAGING_DIR, 'extracted_data.csv')\n",
        "TRANSFORMED_DATA_PATH = os.path.join(STAGING_DIR, 'transformed_data.csv')\n",
        "\n",
        "\n",
        "@dag(\n",
        "    dag_id='etl_pipeline_dag',\n",
        "    start_date=datetime(2023, 1, 1),\n",
        "    schedule_interval=None,\n",
        "    catchup=False,\n",
        "    tags=['etl', 'assignment'],\n",
        ")\n",
        "def etl_pipeline():\n",
        "    \"\"\"\n",
        "    A DAG to perform an ETL process: download, untar, extract, consolidate, and transform data.\n",
        "    \"\"\"\n",
        "    create_staging_dir = BashOperator(\n",
        "        task_id='create_staging_dir',\n",
        "        bash_command=f\"mkdir -p {STAGING_DIR}\"\n",
        "    )\n",
        "\n",
        "    download_data = BashOperator(\n",
        "        task_id='download_data',\n",
        "        bash_command=f\"wget '{DOWNLOAD_URL}' -O {DOWNLOADED_FILE}\"\n",
        "    )\n",
        "\n",
        "    untar_data = BashOperator(\n",
        "        task_id='untar_data',\n",
        "        bash_command=f\"tar -xvzf {DOWNLOADED_FILE} -C {EXTRACTED_DIR}\"\n",
        "    )\n",
        "\n",
        "    @task(task_id='extract_from_csv')\n",
        "    def extract_from_csv():\n",
        "        \"\"\"Extracts required columns from vehicle-data.csv.\"\"\"\n",
        "        # Define the column names since the file has no header\n",
        "        csv_columns = ['Rowid', 'Timestamp', 'Anonymized Vehicle number', 'Vehicle type']\n",
        "\n",
        "        # Read the CSV, specifying no header and providing our own column names\n",
        "        df = pd.read_csv(\n",
        "            os.path.join(EXTRACTED_DIR, 'vehicle-data.csv'),\n",
        "            header=None,\n",
        "            names=csv_columns\n",
        "        )\n",
        "\n",
        "        # The rest of the function is now correct\n",
        "        subset = df[['Rowid', 'Timestamp', 'Anonymized Vehicle number', 'Vehicle type']]\n",
        "        subset.to_csv(CSV_DATA_PATH, index=False)\n",
        "\n",
        "    @task(task_id='extract_from_tsv')\n",
        "    def extract_from_tsv():\n",
        "        \"\"\"Extracts required columns from tollplaza-data.tsv.\"\"\"\n",
        "        # Define the column names since the file has no header\n",
        "        tsv_columns = ['Number of axles', 'Tollplaza id', 'Tollplaza code']\n",
        "\n",
        "        # Read the TSV, specifying no header and providing our own column names\n",
        "        df = pd.read_csv(\n",
        "            os.path.join(EXTRACTED_DIR, 'tollplaza-data.tsv'),\n",
        "            sep='\\t',\n",
        "            header=None,\n",
        "            names=tsv_columns\n",
        "        )\n",
        "\n",
        "        # The rest of the function is now correct\n",
        "        subset = df[['Number of axles', 'Tollplaza id', 'Tollplaza code']]\n",
        "        subset.to_csv(TSV_DATA_PATH, index=False)\n",
        "\n",
        "    @task(task_id='extract_from_fixed_width')\n",
        "    def extract_from_fixed_width():\n",
        "        \"\"\"Extracts data from payment-data.txt using fixed-width positions.\"\"\"\n",
        "        col_specs = [(59, 61), (62, 68)]\n",
        "        col_names = ['Type of Payment code', 'Vehicle Code']\n",
        "        df = pd.read_fwf(os.path.join(EXTRACTED_DIR, 'payment-data.txt'), colspecs=col_specs, header=None, names=col_names)\n",
        "        df.to_csv(FIXED_WIDTH_DATA_PATH, index=False)\n",
        "\n",
        "    @task(task_id='consolidate_data')\n",
        "    def consolidate_data():\n",
        "        \"\"\"Combines the three extracted data files into one.\"\"\"\n",
        "        df_csv = pd.read_csv(CSV_DATA_PATH)\n",
        "        df_tsv = pd.read_csv(TSV_DATA_PATH)\n",
        "        df_fixed = pd.read_csv(FIXED_WIDTH_DATA_PATH)\n",
        "        consolidated_df = pd.concat([df_csv, df_tsv, df_fixed], axis=1)\n",
        "        consolidated_df.to_csv(CONSOLIDATED_DATA_PATH, index=False)\n",
        "\n",
        "    @task(task_id='transform_data')\n",
        "    def transform_data():\n",
        "        \"\"\"Transforms the 'Vehicle type' column to uppercase.\"\"\"\n",
        "        df = pd.read_csv(CONSOLIDATED_DATA_PATH)\n",
        "        df['Vehicle type'] = df['Vehicle type'].str.upper()\n",
        "        df.to_csv(TRANSFORMED_DATA_PATH, index=False)\n",
        "\n",
        "    # Define task dependencies\n",
        "    extraction_tasks = [extract_from_csv(), extract_from_tsv(), extract_from_fixed_width()]\n",
        "\n",
        "    create_staging_dir >> download_data >> untar_data\n",
        "    untar_data >> extraction_tasks\n",
        "    extraction_tasks >> consolidate_data() >> transform_data()\n",
        "\n",
        "etl_dag = etl_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7p7sJg1RDwjD",
        "outputId": "91ec2719-5cb4-40d5-c61e-940b44a6471c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/airflow/dags/etl_pipeline_dag.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Test the DAG Execution\n",
        "\n",
        "This step triggers the pipeline. Since we are not running the full Airflow scheduler service in Colab, we use the airflow dags test command. This powerful command-line tool allows you to execute a single, isolated run of your DAG from start to finish. It's the perfect way to test, debug, and validate your pipeline's logic within a temporary environment like Colab. You will see detailed logs from Airflow as it moves through each task defined in the DAG."
      ],
      "metadata": {
        "id": "0Bw7sKOrFDzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute a test run of the DAG.\n",
        "# We provide a dummy execution date.\n",
        "!airflow dags test etl_pipeline_dag 2023-01-01"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-WhI9PFFLpl",
        "outputId": "137fbb7a-fd2e-4b3d-fa37-e38edb109881"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\u001b[34m2025-05-23T21:50:42.981+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m538} INFO\u001b[0m - Filling up the DagBag from \u001b[01m/content/airflow/dags\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:43.379+0000\u001b[0m] {\u001b[34mutils.py:\u001b[0m162} INFO\u001b[0m - NumExpr defaulting to 2 threads.\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:43.681+0000\u001b[0m] {\u001b[34mexample_python_decorator.py:\u001b[0m79} \u001b[33mWARNING\u001b[0m - \u001b[33mThe virtalenv_python example task requires virtualenv, please install it.\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:43.694+0000\u001b[0m] {\u001b[34mexample_python_operator.py:\u001b[0m88} \u001b[33mWARNING\u001b[0m - \u001b[33mThe virtalenv_python example task requires virtualenv, please install it.\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:43.704+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m348} \u001b[31mERROR\u001b[0m - \u001b[31mFailed to import: \u001b[01m/usr/local/lib/python3.11/dist-packages/airflow/example_dags/example_branch_operator_decorator.py\u001b[22m\u001b[0m\n",
            "\u001b[31mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/dagbag.py\", line 344, in parse\n",
            "    loader.exec_module(new_module)\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/example_dags/example_branch_operator_decorator.py\", line 125, in <module>\n",
            "    random_choice_venv = branching_virtualenv(choices=options)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/decorators/base.py\", line 366, in __call__\n",
            "    op = self.operator_class(\n",
            "         ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/baseoperator.py\", line 437, in apply_defaults\n",
            "    result = func(self, **kwargs, default_args=default_args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/baseoperator.py\", line 437, in apply_defaults\n",
            "    result = func(self, **kwargs, default_args=default_args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/decorators/python.py\", line 52, in __init__\n",
            "    super().__init__(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/baseoperator.py\", line 437, in apply_defaults\n",
            "    result = func(self, **kwargs, default_args=default_args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/decorators/base.py\", line 233, in __init__\n",
            "    super().__init__(task_id=task_id, **kwargs_to_upstream, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/baseoperator.py\", line 437, in apply_defaults\n",
            "    result = func(self, **kwargs, default_args=default_args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/baseoperator.py\", line 437, in apply_defaults\n",
            "    result = func(self, **kwargs, default_args=default_args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/operators/python.py\", line 584, in __init__\n",
            "    raise AirflowException(\"PythonVirtualenvOperator requires virtualenv, please install it.\")\n",
            "airflow.exceptions.AirflowException: PythonVirtualenvOperator requires virtualenv, please install it.\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:43.996+0000\u001b[0m] {\u001b[34mtutorial_taskflow_api_virtualenv.py:\u001b[0m29} \u001b[33mWARNING\u001b[0m - \u001b[33mThe tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.005+0000\u001b[0m] {\u001b[34mdagbag.py:\u001b[0m348} \u001b[31mERROR\u001b[0m - \u001b[31mFailed to import: \u001b[01m/usr/local/lib/python3.11/dist-packages/airflow/example_dags/example_branch_operator.py\u001b[22m\u001b[0m\n",
            "\u001b[31mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/dagbag.py\", line 344, in parse\n",
            "    loader.exec_module(new_module)\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/example_dags/example_branch_operator.py\", line 138, in <module>\n",
            "    branching_venv = BranchPythonVirtualenvOperator(\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/baseoperator.py\", line 437, in apply_defaults\n",
            "    result = func(self, **kwargs, default_args=default_args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/models/baseoperator.py\", line 437, in apply_defaults\n",
            "    result = func(self, **kwargs, default_args=default_args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/operators/python.py\", line 584, in __init__\n",
            "    raise AirflowException(\"PythonVirtualenvOperator requires virtualenv, please install it.\")\n",
            "airflow.exceptions.AirflowException: PythonVirtualenvOperator requires virtualenv, please install it.\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.016+0000\u001b[0m] {\u001b[34mexample_kubernetes_executor.py:\u001b[0m38} \u001b[33mWARNING\u001b[0m - \u001b[33mThe example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes]\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.022+0000\u001b[0m] {\u001b[34mexample_local_kubernetes_executor.py:\u001b[0m39} \u001b[33mWARNING\u001b[0m - \u001b[33mCould not import DAGs in example_local_kubernetes_executor.py\u001b[0m\n",
            "\u001b[33mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/airflow/example_dags/example_local_kubernetes_executor.py\", line 37, in <module>\n",
            "    from kubernetes.client import models as k8s\n",
            "ModuleNotFoundError: No module named 'kubernetes'\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.023+0000\u001b[0m] {\u001b[34mexample_local_kubernetes_executor.py:\u001b[0m40} \u001b[33mWARNING\u001b[0m - \u001b[33mInstall Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes]\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.320+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4069} INFO\u001b[0m - dagrun id: \u001b[01metl_pipeline_dag\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.375+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4085} INFO\u001b[0m - created dagrun \u001b[01m<DagRun etl_pipeline_dag @ 2023-01-01 00:00:00+00:00: manual__2023-01-01T00:00:00+00:00, state:running, queued_at: None. externally triggered: False>\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.419+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01mcreate_staging_dir\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.420+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.create_staging_dir manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,507] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='create_staging_dir' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:44.507+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='create_staging_dir' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,513] {subprocess.py:63} INFO - Tmp dir root location: /tmp\n",
            "[\u001b[34m2025-05-23T21:50:44.513+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m63} INFO\u001b[0m - Tmp dir root location: \u001b[01m/tmp\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,514] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'mkdir -p /content/airflow/dags/staging']\n",
            "[\u001b[34m2025-05-23T21:50:44.514+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m75} INFO\u001b[0m - Running command: \u001b[01m['/usr/bin/bash', '-c', 'mkdir -p /content/airflow/dags/staging']\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,535] {subprocess.py:86} INFO - Output:\n",
            "[\u001b[34m2025-05-23T21:50:44.535+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m86} INFO\u001b[0m - Output:\u001b[0m\n",
            "[2025-05-23 21:50:44,538] {subprocess.py:97} INFO - Command exited with return code 0\n",
            "[\u001b[34m2025-05-23T21:50:44.538+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m97} INFO\u001b[0m - Command exited with return code 0\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.567+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01mcreate_staging_dir\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215044\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.582+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01mcreate_staging_dir\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.613+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01mdownload_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:44.613+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.download_data manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,663] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='download_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:44.663+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='download_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,664] {subprocess.py:63} INFO - Tmp dir root location: /tmp\n",
            "[\u001b[34m2025-05-23T21:50:44.664+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m63} INFO\u001b[0m - Tmp dir root location: \u001b[01m/tmp\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,665] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', \"wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/tolldata.tgz' -O /content/airflow/dags/staging/tolldata.tgz\"]\n",
            "[\u001b[34m2025-05-23T21:50:44.665+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m75} INFO\u001b[0m - Running command: \u001b[01m['/usr/bin/bash', '-c', \"wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/tolldata.tgz' -O /content/airflow/dags/staging/tolldata.tgz\"]\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,685] {subprocess.py:86} INFO - Output:\n",
            "[\u001b[34m2025-05-23T21:50:44.685+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m86} INFO\u001b[0m - Output:\u001b[0m\n",
            "[2025-05-23 21:50:44,691] {subprocess.py:93} INFO - --2025-05-23 21:50:44--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/tolldata.tgz\n",
            "[\u001b[34m2025-05-23T21:50:44.691+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m--2025-05-23 21:50:44--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/tolldata.tgz\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,820] {subprocess.py:93} INFO - Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
            "[\u001b[34m2025-05-23T21:50:44.820+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mResolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:44,889] {subprocess.py:93} INFO - Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
            "[\u001b[34m2025-05-23T21:50:44.889+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mConnecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,224] {subprocess.py:93} INFO - HTTP request sent, awaiting response... 200 OK\n",
            "[\u001b[34m2025-05-23T21:50:45.224+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mHTTP request sent, awaiting response... 200 OK\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,225] {subprocess.py:93} INFO - Length: 528994 (517K) [application/x-tar]\n",
            "[\u001b[34m2025-05-23T21:50:45.225+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mLength: 528994 (517K) [application/x-tar]\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,225] {subprocess.py:93} INFO - Saving to: ‘/content/airflow/dags/staging/tolldata.tgz’\n",
            "[\u001b[34m2025-05-23T21:50:45.225+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mSaving to: ‘/content/airflow/dags/staging/tolldata.tgz’\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,226] {subprocess.py:93} INFO - \n",
            "[\u001b[34m2025-05-23T21:50:45.226+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,364] {subprocess.py:93} INFO -      0K .......... .......... .......... .......... ..........  9%  363K 1s\n",
            "[\u001b[34m2025-05-23T21:50:45.364+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m     0K .......... .......... .......... .......... ..........  9%  363K 1s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,411] {subprocess.py:93} INFO -     50K .......... .......... .......... .......... .......... 19% 1.04M 1s\n",
            "[\u001b[34m2025-05-23T21:50:45.411+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m    50K .......... .......... .......... .......... .......... 19% 1.04M 1s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,454] {subprocess.py:93} INFO -    100K .......... .......... .......... .......... .......... 29% 1.12M 1s\n",
            "[\u001b[34m2025-05-23T21:50:45.454+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   100K .......... .......... .......... .......... .......... 29% 1.12M 1s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,466] {subprocess.py:93} INFO -    150K .......... .......... .......... .......... .......... 38% 4.13M 0s\n",
            "[\u001b[34m2025-05-23T21:50:45.466+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   150K .......... .......... .......... .......... .......... 38% 4.13M 0s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,496] {subprocess.py:93} INFO -    200K .......... .......... .......... .......... .......... 48% 1.59M 0s\n",
            "[\u001b[34m2025-05-23T21:50:45.496+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   200K .......... .......... .......... .......... .......... 48% 1.59M 0s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,521] {subprocess.py:93} INFO -    250K .......... .......... .......... .......... .......... 58% 2.95M 0s\n",
            "[\u001b[34m2025-05-23T21:50:45.521+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   250K .......... .......... .......... .......... .......... 58% 2.95M 0s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,529] {subprocess.py:93} INFO -    300K .......... .......... .......... .......... .......... 67% 2.96M 0s\n",
            "[\u001b[34m2025-05-23T21:50:45.529+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   300K .......... .......... .......... .......... .......... 67% 2.96M 0s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,539] {subprocess.py:93} INFO -    350K .......... .......... .......... .......... .......... 77% 5.42M 0s\n",
            "[\u001b[34m2025-05-23T21:50:45.539+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   350K .......... .......... .......... .......... .......... 77% 5.42M 0s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,551] {subprocess.py:93} INFO -    400K .......... .......... .......... .......... .......... 87% 3.96M 0s\n",
            "[\u001b[34m2025-05-23T21:50:45.551+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   400K .......... .......... .......... .......... .......... 87% 3.96M 0s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,562] {subprocess.py:93} INFO -    450K .......... .......... .......... .......... .......... 96% 4.25M 0s\n",
            "[\u001b[34m2025-05-23T21:50:45.562+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   450K .......... .......... .......... .......... .......... 96% 4.25M 0s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,564] {subprocess.py:93} INFO -    500K .......... ......                                     100% 9.27M=0.3s\n",
            "[\u001b[34m2025-05-23T21:50:45.564+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m   500K .......... ......                                     100% 9.27M=0.3s\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,565] {subprocess.py:93} INFO - \n",
            "[\u001b[34m2025-05-23T21:50:45.565+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,565] {subprocess.py:93} INFO - 2025-05-23 21:50:45 (1.49 MB/s) - ‘/content/airflow/dags/staging/tolldata.tgz’ saved [528994/528994]\n",
            "[\u001b[34m2025-05-23T21:50:45.565+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m2025-05-23 21:50:45 (1.49 MB/s) - ‘/content/airflow/dags/staging/tolldata.tgz’ saved [528994/528994]\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,565] {subprocess.py:93} INFO - \n",
            "[\u001b[34m2025-05-23T21:50:45.565+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,568] {subprocess.py:97} INFO - Command exited with return code 0\n",
            "[\u001b[34m2025-05-23T21:50:45.568+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m97} INFO\u001b[0m - Command exited with return code 0\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.589+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01mdownload_data\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215045\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.604+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01mdownload_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.630+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01muntar_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.630+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.untar_data manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,671] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='untar_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:45.671+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='untar_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,672] {subprocess.py:63} INFO - Tmp dir root location: /tmp\n",
            "[\u001b[34m2025-05-23T21:50:45.672+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m63} INFO\u001b[0m - Tmp dir root location: \u001b[01m/tmp\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,673] {subprocess.py:75} INFO - Running command: ['/usr/bin/bash', '-c', 'tar -xvzf /content/airflow/dags/staging/tolldata.tgz -C /content/airflow/dags/staging']\n",
            "[\u001b[34m2025-05-23T21:50:45.673+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m75} INFO\u001b[0m - Running command: \u001b[01m['/usr/bin/bash', '-c', 'tar -xvzf /content/airflow/dags/staging/tolldata.tgz -C /content/airflow/dags/staging']\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,698] {subprocess.py:86} INFO - Output:\n",
            "[\u001b[34m2025-05-23T21:50:45.698+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m86} INFO\u001b[0m - Output:\u001b[0m\n",
            "[2025-05-23 21:50:45,709] {subprocess.py:93} INFO - fileformats.txt\n",
            "[\u001b[34m2025-05-23T21:50:45.709+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mfileformats.txt\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,709] {subprocess.py:93} INFO - payment-data.txt\n",
            "[\u001b[34m2025-05-23T21:50:45.709+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mpayment-data.txt\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,721] {subprocess.py:93} INFO - ._tollplaza-data.tsv\n",
            "[\u001b[34m2025-05-23T21:50:45.721+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m._tollplaza-data.tsv\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,722] {subprocess.py:93} INFO - tollplaza-data.tsv\n",
            "[\u001b[34m2025-05-23T21:50:45.722+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mtollplaza-data.tsv\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,730] {subprocess.py:93} INFO - ._vehicle-data.csv\n",
            "[\u001b[34m2025-05-23T21:50:45.730+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01m._vehicle-data.csv\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,731] {subprocess.py:93} INFO - vehicle-data.csv\n",
            "[\u001b[34m2025-05-23T21:50:45.731+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m93} INFO\u001b[0m - \u001b[01mvehicle-data.csv\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,739] {subprocess.py:97} INFO - Command exited with return code 0\n",
            "[\u001b[34m2025-05-23T21:50:45.739+0000\u001b[0m] {\u001b[34msubprocess.py:\u001b[0m97} INFO\u001b[0m - Command exited with return code 0\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.760+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01muntar_data\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215045\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.773+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01muntar_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.800+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01mextract_from_tsv\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.801+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.extract_from_tsv manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,843] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='extract_from_tsv' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:45.843+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='extract_from_tsv' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:45,945] {python.py:201} INFO - Done. Returned value was: None\n",
            "[\u001b[34m2025-05-23T21:50:45.945+0000\u001b[0m] {\u001b[34mpython.py:\u001b[0m201} INFO\u001b[0m - Done. Returned value was: \u001b[01mNone\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.950+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01mextract_from_tsv\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215045\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.964+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01mextract_from_tsv\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.965+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01mextract_from_fixed_width\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:45.965+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.extract_from_fixed_width manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,007] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='extract_from_fixed_width' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:46.007+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='extract_from_fixed_width' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,064] {python.py:201} INFO - Done. Returned value was: None\n",
            "[\u001b[34m2025-05-23T21:50:46.064+0000\u001b[0m] {\u001b[34mpython.py:\u001b[0m201} INFO\u001b[0m - Done. Returned value was: \u001b[01mNone\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.069+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01mextract_from_fixed_width\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215046\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.083+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01mextract_from_fixed_width\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.084+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01mextract_from_csv\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.084+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.extract_from_csv manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,127] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='extract_from_csv' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:46.127+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='extract_from_csv' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,203] {python.py:201} INFO - Done. Returned value was: None\n",
            "[\u001b[34m2025-05-23T21:50:46.203+0000\u001b[0m] {\u001b[34mpython.py:\u001b[0m201} INFO\u001b[0m - Done. Returned value was: \u001b[01mNone\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.206+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01mextract_from_csv\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215046\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.217+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01mextract_from_csv\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.236+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01mconsolidate_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.236+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.consolidate_data manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,269] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='consolidate_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:46.269+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='consolidate_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,316] {python.py:201} INFO - Done. Returned value was: None\n",
            "[\u001b[34m2025-05-23T21:50:46.316+0000\u001b[0m] {\u001b[34mpython.py:\u001b[0m201} INFO\u001b[0m - Done. Returned value was: \u001b[01mNone\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.319+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01mconsolidate_data\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215046\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.330+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01mconsolidate_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.349+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4031} INFO\u001b[0m - [DAG TEST] starting task_id=\u001b[01mtransform_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.349+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4034} INFO\u001b[0m - [DAG TEST] running task \u001b[01m<TaskInstance: etl_pipeline_dag.transform_data manual__2023-01-01T00:00:00+00:00 [scheduled]>\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,381] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='transform_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\n",
            "[\u001b[34m2025-05-23T21:50:46.381+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m2480} INFO\u001b[0m - Exporting env vars: \u001b[01mAIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='etl_pipeline_dag' AIRFLOW_CTX_TASK_ID='transform_data' AIRFLOW_CTX_EXECUTION_DATE='2023-01-01T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-01-01T00:00:00+00:00'\u001b[22m\u001b[0m\n",
            "[2025-05-23 21:50:46,436] {python.py:201} INFO - Done. Returned value was: None\n",
            "[\u001b[34m2025-05-23T21:50:46.436+0000\u001b[0m] {\u001b[34mpython.py:\u001b[0m201} INFO\u001b[0m - Done. Returned value was: \u001b[01mNone\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.439+0000\u001b[0m] {\u001b[34mtaskinstance.py:\u001b[0m1138} INFO\u001b[0m - \u001b[01m\u001b[22mMarking task as \u001b[01mSUCCESS\u001b[22m. dag_id=\u001b[01metl_pipeline_dag\u001b[22m, task_id=\u001b[01mtransform_data\u001b[22m, execution_date=\u001b[01m20230101T000000\u001b[22m, start_date=\u001b[01m\u001b[22m, end_date=\u001b[01m20250523T215046\u001b[22m\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.451+0000\u001b[0m] {\u001b[34mdag.py:\u001b[0m4045} INFO\u001b[0m - [DAG TEST] end task task_id=\u001b[01mtransform_data\u001b[22m map_index=-1\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.455+0000\u001b[0m] {\u001b[34mdagrun.py:\u001b[0m732} INFO\u001b[0m - Marking run \u001b[01m<DagRun etl_pipeline_dag @ 2023-01-01 00:00:00+00:00: manual__2023-01-01T00:00:00+00:00, state:running, queued_at: None. externally triggered: False>\u001b[22m successful\u001b[0m\n",
            "[\u001b[34m2025-05-23T21:50:46.456+0000\u001b[0m] {\u001b[34mdagrun.py:\u001b[0m783} INFO\u001b[0m - DagRun Finished: dag_id=\u001b[01metl_pipeline_dag\u001b[22m, execution_date=\u001b[01m2023-01-01 00:00:00+00:00\u001b[22m, run_id=\u001b[01mmanual__2023-01-01T00:00:00+00:00\u001b[22m, run_start_date=\u001b[01m2023-01-01 00:00:00+00:00\u001b[22m, run_end_date=\u001b[01m2025-05-23 21:50:46.456075+00:00\u001b[22m, run_duration=75505846.456075, state=\u001b[01msuccess\u001b[22m, external_trigger=False, run_type=\u001b[01mmanual\u001b[22m, data_interval_start=\u001b[01m2023-01-01 00:00:00+00:00\u001b[22m, data_interval_end=\u001b[01m2023-01-01 00:00:00+00:00\u001b[22m, dag_hash=\u001b[01mNone\u001b[22m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Verify the Final Output\n",
        "\n",
        "The final step is to confirm that your pipeline executed successfully and produced the correct result. First, we list the contents of the staging directory to ensure that all intermediate files and the final output file, transformed_data.csv, were created. Then, we use the pandas library to load the final CSV file into a DataFrame. By displaying the first few rows and checking the unique values in the Vehicle type column, we can visually verify that the data was not only processed but also correctly transformed to uppercase as required by the assignment."
      ],
      "metadata": {
        "id": "EtV0A5V6FLzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the contents of the staging directory to see all the created files\n",
        "!ls -l /content/airflow/dags/staging\n",
        "\n",
        "# Use pandas to read and display the head of the final transformed file\n",
        "import pandas as pd\n",
        "\n",
        "final_output_path = '/content/airflow/dags/staging/transformed_data.csv'\n",
        "\n",
        "try:\n",
        "    final_df = pd.read_csv(final_output_path)\n",
        "    print(\"\\nSuccessfully read the final file. Displaying the first 5 rows:\")\n",
        "    display(final_df.head())\n",
        "\n",
        "    # Verify the transformation\n",
        "    print(\"\\nVerifying the 'Vehicle type' column is in uppercase:\")\n",
        "    print(final_df['Vehicle type'].unique())\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nError: The final output file was not found at {final_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "A1JdJT3kFdLe",
        "outputId": "3bdd18e3-f0e2-4450-a52d-4d5054e8ea86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 3656\n",
            "-rw-r--r-- 1 root root  203687 May 23 21:50 csv_data.csv\n",
            "-rw-r--r-- 1 root root  463765 May 23 21:50 extracted_data.csv\n",
            "-rw-r--r-- 1  501 staff   1704 Aug 22  2021 fileformats.txt\n",
            "-rw-r--r-- 1 root root   90034 May 23 21:50 fixed_width_data.csv\n",
            "-rw-r--r-- 1  501 staff 680000 Aug 22  2021 payment-data.txt\n",
            "-rw-r--r-- 1 root root  528994 Sep 21  2022 tolldata.tgz\n",
            "-rw-r--r-- 1  501 staff 602524 Aug 22  2021 tollplaza-data.tsv\n",
            "-rw-r--r-- 1 root root  463765 May 23 21:50 transformed_data.csv\n",
            "-rw-r--r-- 1 root root  170044 May 23 21:50 tsv_data.csv\n",
            "-rw-r--r-- 1  501 staff 512524 Aug 22  2021 vehicle-data.csv\n",
            "\n",
            "Successfully read the final file. Displaying the first 5 rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Rowid Timestamp  Anonymized Vehicle number Vehicle type  Number of axles  \\\n",
              "0   125094       car                          2        VC965                2   \n",
              "1   174434       car                          2        VC965                2   \n",
              "2  8538286       car                          2        VC965                2   \n",
              "3  5521221       car                          2        VC965                2   \n",
              "4  3267767       car                          2        VC965                2   \n",
              "\n",
              "   Tollplaza id Tollplaza code Type of Payment code Vehicle Code  \n",
              "0          4856      PC7C042B7                   TE        VC965  \n",
              "1          4154      PC2C2EF9E                   TP        VC965  \n",
              "2          4070      PCEECA8B2                   TE        VC965  \n",
              "3          4095      PC3E1512A                   TP        VC965  \n",
              "4          4135      PCC943ECD                   TE        VC965  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3e7c20c3-51f3-44db-999f-a7aa0b76cbaa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rowid</th>\n",
              "      <th>Timestamp</th>\n",
              "      <th>Anonymized Vehicle number</th>\n",
              "      <th>Vehicle type</th>\n",
              "      <th>Number of axles</th>\n",
              "      <th>Tollplaza id</th>\n",
              "      <th>Tollplaza code</th>\n",
              "      <th>Type of Payment code</th>\n",
              "      <th>Vehicle Code</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>125094</td>\n",
              "      <td>car</td>\n",
              "      <td>2</td>\n",
              "      <td>VC965</td>\n",
              "      <td>2</td>\n",
              "      <td>4856</td>\n",
              "      <td>PC7C042B7</td>\n",
              "      <td>TE</td>\n",
              "      <td>VC965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>174434</td>\n",
              "      <td>car</td>\n",
              "      <td>2</td>\n",
              "      <td>VC965</td>\n",
              "      <td>2</td>\n",
              "      <td>4154</td>\n",
              "      <td>PC2C2EF9E</td>\n",
              "      <td>TP</td>\n",
              "      <td>VC965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8538286</td>\n",
              "      <td>car</td>\n",
              "      <td>2</td>\n",
              "      <td>VC965</td>\n",
              "      <td>2</td>\n",
              "      <td>4070</td>\n",
              "      <td>PCEECA8B2</td>\n",
              "      <td>TE</td>\n",
              "      <td>VC965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5521221</td>\n",
              "      <td>car</td>\n",
              "      <td>2</td>\n",
              "      <td>VC965</td>\n",
              "      <td>2</td>\n",
              "      <td>4095</td>\n",
              "      <td>PC3E1512A</td>\n",
              "      <td>TP</td>\n",
              "      <td>VC965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3267767</td>\n",
              "      <td>car</td>\n",
              "      <td>2</td>\n",
              "      <td>VC965</td>\n",
              "      <td>2</td>\n",
              "      <td>4135</td>\n",
              "      <td>PCC943ECD</td>\n",
              "      <td>TE</td>\n",
              "      <td>VC965</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3e7c20c3-51f3-44db-999f-a7aa0b76cbaa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3e7c20c3-51f3-44db-999f-a7aa0b76cbaa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3e7c20c3-51f3-44db-999f-a7aa0b76cbaa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-de4ba640-de43-4c99-81b9-6da6328d3bbd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-de4ba640-de43-4c99-81b9-6da6328d3bbd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-de4ba640-de43-4c99-81b9-6da6328d3bbd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(f\\\"\\\\nError: The final output file was not found at {final_output_path}\\\")\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Rowid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3604502,\n        \"min\": 125094,\n        \"max\": 8538286,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          174434,\n          3267767,\n          8538286\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Timestamp\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"car\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Anonymized Vehicle number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vehicle type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"VC965\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Number of axles\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tollplaza id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 333,\n        \"min\": 4070,\n        \"max\": 4856,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4154\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Tollplaza code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"PC2C2EF9E\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Type of Payment code\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"TP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vehicle Code\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"VC965\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verifying the 'Vehicle type' column is in uppercase:\n",
            "['VC965' 'VCD2F' 'VCB43']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wgLvFCNrGpQD"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}