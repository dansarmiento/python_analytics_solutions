{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlmf8ip97s/m7CEj2g0VT/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dansarmiento/python_analytics_solutions/blob/main/Spark_feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use the feature extractor CountVectorizer\n",
        "- Use the feature extractor TF-IDF\n",
        "- Use the feature transformer Tokenizer\n",
        "- Use the feature transformer StopWordsRemover\n",
        "- Use the feature transformer StringIndexer\n",
        "- Use the feature transformer StandardScaler"
      ],
      "metadata": {
        "id": "JuK4SQq28b_F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5OA6Mxe8R6p",
        "outputId": "b77dad67-dcd0-4057-ac60-523c1d6f9f27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark==3.5 -q\n",
        "!pip install findspark -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# FindSpark simplifies the process of using Apache Spark with Python\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import rand"
      ],
      "metadata": {
        "id": "ns7QC7yV8nMJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create SparkSession\n",
        "#Ignore any warnings by SparkSession command\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Feature Extraction and Transformation using Spark\").getOrCreate()"
      ],
      "metadata": {
        "id": "cbuccI1Q8nOv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "A tokenizer is used to break a sentence into words.\n"
      ],
      "metadata": {
        "id": "ChXhacN08q-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import tokenizer\n",
        "from pyspark.ml.feature import Tokenizer"
      ],
      "metadata": {
        "id": "WejZ6ZtE8nRU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a sample dataframe\n",
        "sentenceDataFrame = spark.createDataFrame([\n",
        "    (1, \"Spark is a distributed computing system.\"),\n",
        "    (2, \"It provides interfaces for multiple languages\"),\n",
        "    (3, \"Spark is built on top of Hadoop\")\n",
        "], [\"id\", \"sentence\"])"
      ],
      "metadata": {
        "id": "ObeDdbUK8nTq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the dataframe\n",
        "sentenceDataFrame.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEjr6DR78nWH",
        "outputId": "832eb61a-71b3-4cb1-bf34-9253b909fbbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------------------+\n",
            "|id |sentence                                     |\n",
            "+---+---------------------------------------------+\n",
            "|1  |Spark is a distributed computing system.     |\n",
            "|2  |It provides interfaces for multiple languages|\n",
            "|3  |Spark is built on top of Hadoop              |\n",
            "+---+---------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create tokenizer instance.\n",
        "#mention the column to be tokenized as inputcol\n",
        "#mention the output column name where the tokens are to be stored.\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")"
      ],
      "metadata": {
        "id": "EkfARg9r8nY7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize\n",
        "token_df = tokenizer.transform(sentenceDataFrame)"
      ],
      "metadata": {
        "id": "Fvg0bOT781y3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the tokenized data\n",
        "token_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBu9ftEM811V",
        "outputId": "07afb679-5566-4b08-c073-7e11c8f6801a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------------------+----------------------------------------------------+\n",
            "|id |sentence                                     |words                                               |\n",
            "+---+---------------------------------------------+----------------------------------------------------+\n",
            "|1  |Spark is a distributed computing system.     |[spark, is, a, distributed, computing, system.]     |\n",
            "|2  |It provides interfaces for multiple languages|[it, provides, interfaces, for, multiple, languages]|\n",
            "|3  |Spark is built on top of Hadoop              |[spark, is, built, on, top, of, hadoop]             |\n",
            "+---+---------------------------------------------+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CountVectorizer\n",
        "\n",
        "CountVectorizer is used to convert text into numerical format. It gives the count of each word in a given document."
      ],
      "metadata": {
        "id": "g7rk-1DV848j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import CountVectorizer\n",
        "from pyspark.ml.feature import CountVectorizer"
      ],
      "metadata": {
        "id": "miSHGD7c813x"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a sample dataframe and display it.\n",
        "textdata = [(1, \"I love Spark Spark provides Python API \".split()),\n",
        "            (2, \"I love Python Spark supports Python\".split()),\n",
        "            (3, \"Spark solves the big problem of big data\".split())]\n",
        "\n",
        "textdata = spark.createDataFrame(textdata, [\"id\", \"words\"])\n",
        "\n",
        "textdata.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxjqwUyZ816B",
        "outputId": "62e2bac4-bf6f-45c5-8b45-74eb8874bbd1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------------------+\n",
            "|id |words                                            |\n",
            "+---+-------------------------------------------------+\n",
            "|1  |[I, love, Spark, Spark, provides, Python, API]   |\n",
            "|2  |[I, love, Python, Spark, supports, Python]       |\n",
            "|3  |[Spark, solves, the, big, problem, of, big, data]|\n",
            "+---+-------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CountVectorizer object\n",
        "# mention the column to be count vectorized as inputcol\n",
        "# mention the output column name where the count vectors are to be stored.\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")"
      ],
      "metadata": {
        "id": "akg4BhkC818d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the CountVectorizer model on the input data\n",
        "model = cv.fit(textdata)"
      ],
      "metadata": {
        "id": "mWDv55Rj81-s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the input data to bag-of-words vectors\n",
        "result = model.transform(textdata)"
      ],
      "metadata": {
        "id": "IlSAYLqJ9A2x"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the dataframe\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi9k1elV9A46",
        "outputId": "fba40a6f-3231-42f9-823d-711a4eca0db8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------------------------------------+----------------------------------------------------+\n",
            "|id |words                                            |features                                            |\n",
            "+---+-------------------------------------------------+----------------------------------------------------+\n",
            "|1  |[I, love, Spark, Spark, provides, Python, API]   |(13,[0,1,2,3,6,8],[2.0,1.0,1.0,1.0,1.0,1.0])        |\n",
            "|2  |[I, love, Python, Spark, supports, Python]       |(13,[0,1,2,3,12],[1.0,2.0,1.0,1.0,1.0])             |\n",
            "|3  |[Spark, solves, the, big, problem, of, big, data]|(13,[0,4,5,7,9,10,11],[1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "+---+-------------------------------------------------+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF - Term Frequency-Inverse Document Frequency\n",
        "\n",
        "Term Frequency-Inverse Document Frequency is used to quantify the importance of a word in a document. TF-IDF is computed by multiplying the number of times a word occurs in a document by the inverse document frequency of the word."
      ],
      "metadata": {
        "id": "U8VoAgQB9DrY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary classes for TF-IDF calculation\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer"
      ],
      "metadata": {
        "id": "IWPKNEw99A7J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a sample dataframe and display it.\n",
        "sentenceData = spark.createDataFrame([\n",
        "        (1, \"Spark supports python\"),\n",
        "        (2, \"Spark is fast\"),\n",
        "        (3, \"Spark is easy\")\n",
        "    ], [\"id\", \"sentence\"])\n",
        "\n",
        "sentenceData.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgzGrKOs9A9n",
        "outputId": "99c4ca57-0c1b-48b7-fb71-229832c5cb53"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------+\n",
            "|id |sentence             |\n",
            "+---+---------------------+\n",
            "|1  |Spark supports python|\n",
            "|2  |Spark is fast        |\n",
            "|3  |Spark is easy        |\n",
            "+---+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenize the \"sentence\" column and store in the column \"words\"\n",
        "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
        "wordsData = tokenizer.transform(sentenceData)\n",
        "wordsData.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVlPbmIJ9A_8",
        "outputId": "99ac6238-60a4-47ef-f676-385c6fc099ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------+-------------------------+\n",
            "|id |sentence             |words                    |\n",
            "+---+---------------------+-------------------------+\n",
            "|1  |Spark supports python|[spark, supports, python]|\n",
            "|2  |Spark is fast        |[spark, is, fast]        |\n",
            "|3  |Spark is easy        |[spark, is, easy]        |\n",
            "+---+---------------------+-------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a HashingTF object\n",
        "# mention the \"words\" column as input\n",
        "# mention the \"rawFeatures\" column as output\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=10)\n",
        "featurizedData = hashingTF.transform(wordsData)\n",
        "\n",
        "featurizedData.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7wgZiJh9BCT",
        "outputId": "1b43d05f-cb18-4fc1-b204-f7e6b669f624"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------+-------------------------+--------------------------+\n",
            "|id |sentence             |words                    |rawFeatures               |\n",
            "+---+---------------------+-------------------------+--------------------------+\n",
            "|1  |Spark supports python|[spark, supports, python]|(10,[4,6,9],[1.0,1.0,1.0])|\n",
            "|2  |Spark is fast        |[spark, is, fast]        |(10,[3,6,9],[1.0,1.0,1.0])|\n",
            "|3  |Spark is easy        |[spark, is, easy]        |(10,[0,6,9],[1.0,1.0,1.0])|\n",
            "+---+---------------------+-------------------------+--------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an IDF object\n",
        "# mention the \"rawFeatures\" column as input\n",
        "# mention the \"features\" column as output\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "idfModel = idf.fit(featurizedData)\n",
        "tfidfData = idfModel.transform(featurizedData)"
      ],
      "metadata": {
        "id": "N70QJMBr9BEh"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#display the tf-idf data\n",
        "tfidfData.select(\"sentence\", \"features\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HW-ni7Vc9PbL",
        "outputId": "f983967a-4a5c-478a-b782-279e03911f6a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-----------------------------------------+\n",
            "|sentence             |features                                 |\n",
            "+---------------------+-----------------------------------------+\n",
            "|Spark supports python|(10,[4,6,9],[0.6931471805599453,0.0,0.0])|\n",
            "|Spark is fast        |(10,[3,6,9],[0.6931471805599453,0.0,0.0])|\n",
            "|Spark is easy        |(10,[0,6,9],[0.6931471805599453,0.0,0.0])|\n",
            "+---------------------+-----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StopWordsRemover\n",
        "\n",
        "StopWordsRemover is a transformer that filters out stop words like \"a\",\"an\" and \"the\"."
      ],
      "metadata": {
        "id": "Vnklc6k59Rk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import StopWordsRemover\n",
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "metadata": {
        "id": "wKHsXhmz9Pdw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dataframe with sample text and display it\n",
        "textData = spark.createDataFrame([\n",
        "    (1, ['Spark', 'is', 'an', 'open-source', 'distributed', 'computing', 'system']),\n",
        "    (2, ['IT', 'has', 'interfaces', 'for', 'multiple', 'languages']),\n",
        "    (3, ['It', 'has', 'a', 'wide', 'range', 'of', 'libraries', 'and', 'APIs'])\n",
        "], [\"id\", \"sentence\"])\n",
        "\n",
        "textData.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OQhgiOl9PgG",
        "outputId": "ce667198-6a99-43d1-8e7f-706be7ddb8c0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------------------------------------+\n",
            "|id |sentence                                                    |\n",
            "+---+------------------------------------------------------------+\n",
            "|1  |[Spark, is, an, open-source, distributed, computing, system]|\n",
            "|2  |[IT, has, interfaces, for, multiple, languages]             |\n",
            "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |\n",
            "+---+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords from \"sentence\" column and store the result in \"filtered_sentence\" column\n",
        "remover = StopWordsRemover(inputCol=\"sentence\", outputCol=\"filtered_sentence\")\n",
        "textData = remover.transform(textData)"
      ],
      "metadata": {
        "id": "sYxOmgRb9Pii"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the dataframe\n",
        "textData.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgHENYH99PlB",
        "outputId": "3fee8e77-7cee-4cc2-851c-d1902cf2567a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------------------------------------+----------------------------------------------------+\n",
            "|id |sentence                                                    |filtered_sentence                                   |\n",
            "+---+------------------------------------------------------------+----------------------------------------------------+\n",
            "|1  |[Spark, is, an, open-source, distributed, computing, system]|[Spark, open-source, distributed, computing, system]|\n",
            "|2  |[IT, has, interfaces, for, multiple, languages]             |[interfaces, multiple, languages]                   |\n",
            "|3  |[It, has, a, wide, range, of, libraries, and, APIs]         |[wide, range, libraries, APIs]                      |\n",
            "+---+------------------------------------------------------------+----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StringIndexer\n",
        "\n",
        "StringIndexer converts a column of strings into a column of integers."
      ],
      "metadata": {
        "id": "aWdu6Iqy9Y20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import StringIndexer\n",
        "from pyspark.ml.feature import StringIndexer"
      ],
      "metadata": {
        "id": "3Ud72emh9BGv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a dataframe with sample text and display it\n",
        "colors = spark.createDataFrame(\n",
        "    [(0, \"red\"), (1, \"red\"), (2, \"blue\"), (3, \"yellow\" ), (4, \"yellow\"), (5, \"yellow\")],\n",
        "    [\"id\", \"color\"])\n",
        "\n",
        "colors.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8Wsv1849BJF",
        "outputId": "1f4719a6-f3e9-47bb-8094-2889e826aa32"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id| color|\n",
            "+---+------+\n",
            "|  0|   red|\n",
            "|  1|   red|\n",
            "|  2|  blue|\n",
            "|  3|yellow|\n",
            "|  4|yellow|\n",
            "|  5|yellow|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# index the strings in the column \"color\" and store their indexes in the column \"colorIndex\"\n",
        "indexer = StringIndexer(inputCol=\"color\", outputCol=\"colorIndex\")\n",
        "indexed = indexer.fit(colors).transform(colors)"
      ],
      "metadata": {
        "id": "BBIKhD7z9dWh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display the dataframe\n",
        "indexed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MlLoIzA9dY-",
        "outputId": "d59b94f1-dd56-4dee-b4cb-ebce4cd34d0a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+----------+\n",
            "| id| color|colorIndex|\n",
            "+---+------+----------+\n",
            "|  0|   red|       1.0|\n",
            "|  1|   red|       1.0|\n",
            "|  2|  blue|       2.0|\n",
            "|  3|yellow|       0.0|\n",
            "|  4|yellow|       0.0|\n",
            "|  5|yellow|       0.0|\n",
            "+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StandardScaler  \n",
        "\n",
        "StandardScaler transforms the data so that it has a mean of 0 and a standard deviation of 1"
      ],
      "metadata": {
        "id": "5jTTM33O9fce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import StandardScaler\n",
        "from pyspark.ml.feature import StandardScaler\n"
      ],
      "metadata": {
        "id": "pWAItq7Y9dbT"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataframe and display it\n",
        "from pyspark.ml.linalg import Vectors\n",
        "data = [(1, Vectors.dense([70, 170, 17])),\n",
        "        (2, Vectors.dense([80, 165, 25])),\n",
        "        (3, Vectors.dense([65, 150, 135]))]\n",
        "df = spark.createDataFrame(data, [\"id\", \"features\"])\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on5OktZa9ddq",
        "outputId": "f176bdd7-a9e1-452a-bdcc-11f9dd5e2788"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "| id|          features|\n",
            "+---+------------------+\n",
            "|  1| [70.0,170.0,17.0]|\n",
            "|  2| [80.0,165.0,25.0]|\n",
            "|  3|[65.0,150.0,135.0]|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the StandardScaler transformer\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)"
      ],
      "metadata": {
        "id": "lOrjL1Bt9dgA"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the transformer to the dataset\n",
        "scalerModel = scaler.fit(df)"
      ],
      "metadata": {
        "id": "yyN4NNlq9l5U"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data\n",
        "scaledData = scalerModel.transform(df)"
      ],
      "metadata": {
        "id": "vLW81NGZ9l8B"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the scaled data\n",
        "scaledData.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJVtIdc99l-e",
        "outputId": "3fadd98b-f0b2-4cf6-e847-826a4349a4d2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------------------------------------------------------+\n",
            "|id |features          |scaledFeatures                                              |\n",
            "+---+------------------+------------------------------------------------------------+\n",
            "|1  |[70.0,170.0,17.0] |[-0.218217890235993,0.8006407690254366,-0.6369487984517485] |\n",
            "|2  |[80.0,165.0,25.0] |[1.0910894511799611,0.32025630761017515,-0.5156252177942725]|\n",
            "|3  |[65.0,150.0,135.0]|[-0.8728715609439701,-1.120897076635609,1.152574016246021]  |\n",
            "+---+------------------+------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "P9cIM2Z89mA0"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}