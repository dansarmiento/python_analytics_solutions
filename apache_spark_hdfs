{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPftfwmK7Yk3GaxaAFGUBUk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dansarmiento/analytics_portfolio/blob/main/apache_spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This workbook is an example of doing data transformation with PySpark with the final results being written to a Hive warehouse and HDFS file system"
      ],
      "metadata": {
        "id": "y3EQBih_9VCJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js6iemsr6Jvx",
        "outputId": "42538658-418c-4e7c-a2fa-4803e0754c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet wget pysparkâ€¯ findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "import wget\n",
        "from pyspark.sql.functions import year, quarter, to_date\n",
        "from pyspark.sql.functions import sum\n",
        "from pyspark.sql.functions import when, lit\n",
        "from pyspark.sql.functions import avg"
      ],
      "metadata": {
        "id": "MlCEMhMy6TK0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a SparkContext object\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "\n",
        "# Creating a Spark Session\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark DataFrames basic example\") \\\n",
        "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "xzwUCrRE6iip"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load datasets into PySpark DataFrames**"
      ],
      "metadata": {
        "id": "nehJ8Su962RC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "link_to_data1 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset1.csv'\n",
        "wget.download(link_to_data1)\n",
        "\n",
        "link_to_data2 = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-BD0225EN-SkillsNetwork/labs/data/dataset2.csv'\n",
        "wget.download(link_to_data2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XRL-ndzC6vL3",
        "outputId": "c36f47d4-ffad-49db-af27-85650cade8a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dataset2.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load the data into a pyspark dataframe\n",
        "\n",
        "df1 = spark.read.csv(\"dataset1.csv\", header=True, inferSchema=True)\n",
        "df2 = spark.read.csv(\"dataset2.csv\", header=True, inferSchema=True)"
      ],
      "metadata": {
        "id": "-NGmVox463ae"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display the schema for both dataframes**"
      ],
      "metadata": {
        "id": "2_LUjyxx69ch"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print the schema of df1 and df2\n",
        "\n",
        "df1.printSchema()\n",
        "df2.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epnU82k965z9",
        "outputId": "aad912fa-7334-48da-eff1-eec797ffe288"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- date_column: string (nullable = true)\n",
            " |-- amount: integer (nullable = true)\n",
            " |-- description: string (nullable = true)\n",
            " |-- location: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- customer_id: integer (nullable = true)\n",
            " |-- transaction_date: string (nullable = true)\n",
            " |-- value: integer (nullable = true)\n",
            " |-- notes: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add date component columns**"
      ],
      "metadata": {
        "id": "UEMM_E9L7E8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add new column year to df1\n",
        "df1 = df1.withColumn('year', year(to_date('date_column','dd/MM/yyyy')))\n",
        "\n",
        "#Add new column quarter to df2\n",
        "\n",
        "df2 = df2.withColumn('quarter', quarter(to_date('transaction_date','dd/MM/yyyy')))\n"
      ],
      "metadata": {
        "id": "4Jkeafkk7AeO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Rename columns**"
      ],
      "metadata": {
        "id": "-JtpzJXQ7KW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Rename df1 column amount to transaction_amount\n",
        "df1 = df1.withColumnRenamed('amount', 'transaction_amount')\n",
        "\n",
        "#Rename df2 column value to transaction_value\n",
        "df2 = df2.withColumnRenamed('value', 'transaction_value')\n"
      ],
      "metadata": {
        "id": "KtQJ4CPE7Il-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Drop columns**"
      ],
      "metadata": {
        "id": "ZC4gcfI_7Orn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop columns description and location from df1\n",
        "df1 = df1.drop('description', 'location')\n",
        "\n",
        "#Drop column notes from df2\n",
        "df2 = df2.drop('notes')"
      ],
      "metadata": {
        "id": "zJudc_Bz7NOu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**join dataframes**"
      ],
      "metadata": {
        "id": "QIZlaHjI7SeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#join df1 and df2 based on common column customer_id\n",
        "joined_df = df1.join(df2, 'customer_id', 'inner')"
      ],
      "metadata": {
        "id": "hPph0BhB7RQO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conditional filtering**"
      ],
      "metadata": {
        "id": "oj-C0Va67YKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = joined_df.filter(\"transaction_amount > 1000\")"
      ],
      "metadata": {
        "id": "1-4LKHz37XDr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aggregate**"
      ],
      "metadata": {
        "id": "GQls4vvD7isW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group by customer_id and aggregate the sum of transaction amount\n",
        "total_amount_per_customer = filtered_df.groupBy('customer_id').agg(sum('transaction_amount').alias('total_amount'))\n",
        "\n",
        "#display the result\n",
        "total_amount_per_customer.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YG1GmQYd7dPs",
        "outputId": "70c8e366-2d6a-4f87-b767-60b0a4d5b78d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|customer_id|total_amount|\n",
            "+-----------+------------+\n",
            "|         31|        3200|\n",
            "|         85|        1800|\n",
            "|         78|        1500|\n",
            "|         34|        1200|\n",
            "|         81|        5500|\n",
            "|         28|        2600|\n",
            "|         76|        2600|\n",
            "|         27|        4200|\n",
            "|         91|        3200|\n",
            "|         22|        1200|\n",
            "|         93|        5500|\n",
            "|          1|        5000|\n",
            "|         52|        2600|\n",
            "|         13|        4800|\n",
            "|          6|        4500|\n",
            "|         16|        2600|\n",
            "|         40|        2600|\n",
            "|         94|        1200|\n",
            "|         57|        5500|\n",
            "|         54|        1500|\n",
            "+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write to Hive table**"
      ],
      "metadata": {
        "id": "BBJt7BmR7mcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write total_amount_per_customer to a Hive table named customer_totals\n",
        "total_amount_per_customer.write.mode(\"overwrite\").saveAsTable(\"customer_totals\")"
      ],
      "metadata": {
        "id": "mo8Ew7a-7ky6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write to HDFS**"
      ],
      "metadata": {
        "id": "44mr7O-97rnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write filtered_df to HDFS in parquet format file filtered_data.parquet\n",
        "filtered_df.write.mode(\"overwrite\").parquet(\"filtered_data.parquet\")"
      ],
      "metadata": {
        "id": "tqsEMkx_7paR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add a new conditional column**"
      ],
      "metadata": {
        "id": "cdU7gDmF80rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add new column with value indicating whether transaction amount is > 5000 or not\n",
        "df1 = df1.withColumn(\"high_value\", when(df1.transaction_amount > 5000, lit(\"Yes\")).otherwise(lit(\"No\")))"
      ],
      "metadata": {
        "id": "B1J2yix_7t3p"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Avg transaction value per quarter**"
      ],
      "metadata": {
        "id": "AuRw80gv89NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#calculate the average transaction value for each quarter in df2\n",
        "average_value_per_quarter = df2.groupBy('quarter').agg(avg(\"transaction_value\").alias(\"avg_trans_val\"))\n",
        "\n",
        "\n",
        "#show the average transaction value for each quarter in df2\n",
        "average_value_per_quarter.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-4K4xwx87Op",
        "outputId": "85da6bb7-96c4-4979-d142-92f86480d7e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+\n",
            "|quarter|     avg_trans_val|\n",
            "+-------+------------------+\n",
            "|      1| 1111.111111111111|\n",
            "|      3|1958.3333333333333|\n",
            "|      4| 816.6666666666666|\n",
            "|      2|            1072.0|\n",
            "+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write results to Hive table**"
      ],
      "metadata": {
        "id": "M5Z7q8h49FA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write average_value_per_quarter to a Hive table named quarterly_averages\n",
        "\n",
        "average_value_per_quarter.write.mode(\"overwrite\").saveAsTable(\"quarterly_averages\")"
      ],
      "metadata": {
        "id": "uJETJpH48_7X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculate total transaction value per year**"
      ],
      "metadata": {
        "id": "gKik4d5-9Igo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the total transaction value for each year in df1.\n",
        "total_value_per_year = df1.groupBy('year').agg(sum(\"transaction_amount\").alias(\"total_transaction_val\"))\n",
        "\n",
        "# show the total transaction value for each year in df1.\n",
        "total_value_per_year.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyamZ6Wh9HGF",
        "outputId": "ac6682f8-6db3-4152-d58b-1d5ffa4cb45f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------------------+\n",
            "|year|total_transaction_val|\n",
            "+----+---------------------+\n",
            "|2025|                25700|\n",
            "|2027|                25700|\n",
            "|2023|                28100|\n",
            "|2022|                29800|\n",
            "|2026|                25700|\n",
            "|2029|                25700|\n",
            "|2030|                 9500|\n",
            "|2028|                25700|\n",
            "|2024|                25700|\n",
            "+----+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write result to HDFS**"
      ],
      "metadata": {
        "id": "ERy--uMX9NT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write total_value_per_year to HDFS in the CSV format\n",
        "total_value_per_year.write.mode(\"overwrite\").csv(\"total_value_per_year.csv\")"
      ],
      "metadata": {
        "id": "KLuIedIl9LyK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JUAnJxqi9QVq"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}
