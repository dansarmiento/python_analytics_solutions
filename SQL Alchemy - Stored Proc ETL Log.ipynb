{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e725ae-1a7e-4e32-900b-3039bb46ce87",
   "metadata": {},
   "source": [
    "# SQL Server Agent - Detailed job history\n",
    "\n",
    "This notebook blends sql and python to accomplish transformations with pandas and pulling in file metadata with python.  All of this creates an ending dataframe with comprehensive information in one place about each stored procedure run by each job and what target is being updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b9c2fd1-5c07-4288-b8b5-b3e4be94ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os \n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "import re\n",
    "\n",
    "# Setting this option to evaluate the code stored in a field\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# Database connection details\n",
    "server = '#########'  \n",
    "database = '#########'  \n",
    "username = '#########'  \n",
    "password = '#########'\n",
    "\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "encoded_conn_str = urllib.parse.quote_plus(connection_string)\n",
    "\n",
    "# Create the SQLAlchemy engine \n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={encoded_conn_str}')\n",
    "\n",
    "#\n",
    "query = '''\n",
    "SELECT \n",
    "    SCHEMA_NAME(p.schema_id) as SchemaName\n",
    "    , p.name as ProcedureName\n",
    "    , m.definition as ProcedureDefinition\n",
    "FROM sys.procedures as p\n",
    "inner join sys.sql_modules as m\n",
    "on p.object_id = m.object_id\n",
    "'''\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783099ae-0bf4-4680-a037-e9d3b8c0871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need regex to process raw text for stored procedure stored in df.ProcedureDefinition\n",
    "# df.ProcedureDefinition.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e578651d-2da4-4eab-8f7c-2b06c4aa8c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper to extract ETL info from stored procedure SQL\n",
    "def parse_etl_info_from_sql(proc_name, sql_text):\n",
    "    etl_info_list = []\n",
    "\n",
    "    # Detect all BULK INSERT operations\n",
    "    bulk_inserts = re.findall(r\"BULK\\s+INSERT\\s+(#\\w+)\\s+FROM\\s+'([^']+)'\", sql_text, re.IGNORECASE)\n",
    "\n",
    "    # Detect all MERGE operations\n",
    "    merges = re.findall(r\"MERGE\\s+(\\[.*?\\])\\s+as\\s+tgt\\s+USING\\s+(#\\w+)\", sql_text, re.IGNORECASE)\n",
    "\n",
    "    # Detect all TRUNCATE TABLE operations\n",
    "    truncates = re.findall(r\"TRUNCATE\\s+TABLE\\s+(\\[.*?\\])\", sql_text, re.IGNORECASE)\n",
    "\n",
    "    # Detect all INSERT INTO ... SELECT * FROM #temp_table\n",
    "    inserts = re.findall(r\"INSERT\\s+INTO\\s+(\\[.*?\\])\\s+SELECT\\s+\\*\\s+FROM\\s+(#\\w+)\", sql_text, re.IGNORECASE)\n",
    "\n",
    "    # Process bulk -> insert flow\n",
    "    for temp_table, file_path in bulk_inserts:\n",
    "        # See if that temp table is later used in a MERGE or INSERT\n",
    "        targets = [target for target, source in merges + inserts if source.lower() == temp_table.lower()]\n",
    "        for target in targets:\n",
    "            etl_info_list.append({\n",
    "                'stored_procedure': proc_name,\n",
    "                'source': file_path,\n",
    "                'source_type': 'file',\n",
    "                'target': target.replace('[', '').replace(']', ''),\n",
    "                'process_type': 'merge-join' if (target, temp_table) in merges else 'truncate and load'\n",
    "            })\n",
    "\n",
    "    return etl_info_list\n",
    "\n",
    "\n",
    "\n",
    "# Extract ETL info\n",
    "etl_info_records = []\n",
    "for idx, row in df.iterrows():\n",
    "    records = parse_etl_info_from_sql(row['ProcedureName'], row['ProcedureDefinition'])\n",
    "    etl_info_records.extend(records)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_etl_summary = pd.DataFrame(etl_info_records)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cffad0a1-9f9d-4ec8-88b5-018063e749a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking that we got clean information from the array of stored procedures\n",
    "# df_etl_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe47ff8-707e-40ac-9525-235ccc103d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding source file metadata to the dataframe\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_file_info(file_path):\n",
    "    try:\n",
    "        stats = os.stat(file_path)\n",
    "        return {\n",
    "            'last_modified': datetime.fromtimestamp(stats.st_mtime),\n",
    "            'last_accessed': datetime.fromtimestamp(stats.st_atime),\n",
    "            'file_size_mb': round(stats.st_size / (1024 * 1024), 2)\n",
    "        }\n",
    "    except FileNotFoundError:\n",
    "        return {\n",
    "            'last_modified': None,\n",
    "            'last_accessed': None,\n",
    "            'file_size_mb': None\n",
    "        }\n",
    "\n",
    "# Step 1: Replace vault path with mapped drive\n",
    "df_etl_summary['server_path'] = df_etl_summary['source'].str.replace(\n",
    "    '\\\\\\\\vault\\\\powerbiflatfiles$\\\\production\\\\', 'V:\\\\Production\\\\', regex=False\n",
    ")\n",
    "\n",
    "# Step 2: Normalize file paths\n",
    "df_etl_summary['server_path'] = df_etl_summary['server_path'].apply(os.path.normpath)\n",
    "\n",
    "# Step 3: Apply the file info function\n",
    "file_info_list = df_etl_summary['server_path'].apply(get_file_info)\n",
    "\n",
    "# Step 4: Convert to DataFrame\n",
    "df_file_info = pd.DataFrame(file_info_list.tolist())\n",
    "\n",
    "# Step 5: Merge with original summary\n",
    "df_full = pd.concat([df_etl_summary, df_file_info], axis=1)\n",
    "\n",
    "\n",
    "# df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d013bcf-7dc9-4a09-94fa-27ac05e313ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory efficient approach to get row and column count from files\n",
    "def get_delimited_file_shape(file_path):\n",
    "    try:\n",
    "        # Detect delimiter based on extension\n",
    "        delimiter = ',' if file_path.lower().endswith('.csv') else '|'\n",
    "        \n",
    "        # Open file in text mode\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            header = f.readline()\n",
    "            col_count = len(header.strip().split(delimiter))\n",
    "            row_count = sum(1 for _ in f)  # count remaining lines (i.e., rows)\n",
    "        \n",
    "        return (row_count, col_count)\n",
    "    except Exception:\n",
    "        return (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f380a7f9-b8b7-4220-9066-91384a7c2024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_full[['n_rows', 'n_cols']] = df_full['server_path'].apply(\n",
    "    lambda x: pd.Series(get_delimited_file_shape(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e514a50-e1bf-4399-8a1a-de6a92f1bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results look good and the runtime was also quick\n",
    "# df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f72a4a7-f6c1-4394-a2e3-8c3f6bfbc6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full.to_sql('stored_procedures', con=engine, schema='src', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7495ffa-a863-4f31-b4e4-22003f84015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "engine.dispose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbf75c68-56eb-4123-8e84-486c8143b66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stored procedures run details from msdb\n",
    "# Database connection details \n",
    "server = '#########'  \n",
    "database = '#########'  \n",
    "username = '#########'  \n",
    "password = '#########'\n",
    "\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "encoded_conn_str = urllib.parse.quote_plus(connection_string)\n",
    "\n",
    "# Create the SQLAlchemy engine \n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={encoded_conn_str}')\n",
    "\n",
    "query = '''\n",
    "SELECT  \n",
    "    [job],\n",
    "    [step],\n",
    "    [step_order],\n",
    "    [start_time],\n",
    "    [job_outcome],\n",
    "    [duration_in_seconds],\n",
    "    [error_message],\n",
    "    [command]\n",
    "FROM [msdb].[dbo].[job_step_history_analysis] AS a1\n",
    "WHERE \n",
    "    [job] NOT IN ('syspolicy_purge_history', 'backup db - ucr_health')\n",
    "    AND [start_time] = (\n",
    "        SELECT MAX(a2.[start_time])\n",
    "        FROM [msdb].[dbo].[job_step_history_analysis] AS a2\n",
    "        WHERE a2.[step] = a1.[step]\n",
    "          AND a2.[job] = a1.[job]\n",
    "    );\n",
    "'''\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Close the connection\n",
    "engine.dispose() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa461a8c-4a84-4ad3-86a0-535c704a5093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results came out as expected\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19e60b1c-d31e-4c2f-a79c-5514ee3a19da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    update_fact_smart_tools_log\n",
       "1           update_fact_smartset\n",
       "2                 shrink_temp_db\n",
       "3                 shrink_temp_db\n",
       "4        update_src_availability\n",
       "Name: stored_procedure, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using regex to process the command to get the stored procedure to join to the other dataframe\n",
    "\n",
    "df['stored_procedure'] = (\n",
    "    df['command']\n",
    "    .str.replace(r'[\\[\\];]', '', regex=True)              # remove brackets and semicolon\n",
    "    .str.extract(r'\\.([^.]+)\\s*$', expand=False)          # get string after last dot\n",
    ")\n",
    "df.stored_procedure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c783bc19-c541-41e0-8535-2b062a14a19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging df_full and df on stored_procedure\n",
    "\n",
    "df_joined = pd.merge(\n",
    "    df_full,\n",
    "    df,\n",
    "    on='stored_procedure',\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01bbf555-1c72-4171-81f2-cbce69f44cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results are as expected\n",
    "# df_joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700ebda6-1740-41a7-bcbb-c58cb8fb932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect back to main database to write the table to the server\n",
    "\n",
    "# Database connection details\n",
    "server = '#########'  \n",
    "database = '#########'  \n",
    "username = '#########'  \n",
    "password = '#########'\n",
    "\n",
    "\n",
    "# Construct the connection string\n",
    "connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}'\n",
    "encoded_conn_str = urllib.parse.quote_plus(connection_string)\n",
    "\n",
    "# Create the SQLAlchemy engine \n",
    "engine = create_engine(f'mssql+pyodbc:///?odbc_connect={encoded_conn_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f010e25-4ee9-4da7-b23d-38f36cca35d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the table to the server\n",
    "\n",
    "df_joined.to_sql('stored_procedures', con=engine, schema='src', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "772d183f-1816-42d6-b961-85fdd2458ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the connection\n",
    "engine.dispose() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
